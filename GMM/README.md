# Gaussian Mixture Models

## What are Gaussian Mixture Models?
A **Gaussian Mixture Model** is a probabilistic algorithm which attempts to model data without explicit knowledge about the distribution it has been generated.

GMMs work under the assumption that the distribution can be modelled via the weighted mixture of finite Gaussian.

The GMM is parameterized by three components:
1. The Component Weights
2. Means
3. Variances (Univariate) / Covariance (Multivariate)

In order to work with data which is incomplete and is generated by unobserved latents, we use Expectation Maximization (EM) to iteratively maximize our Log-Likelihood and subsequently update our parameters.

## Expectation Maximization Algorithm

The expectation maximization algorithms is a two step iterative process for working to estimate a distribution with (missing) data or those generated by unobserved latents.

A nice property of the EM algorithm is that it is **strictly increasing** across iterations and guarantees convergence to a **Local Minima**.

Values related to mean, variances are randomly initialized at the start of the process.

In GMMs, we use Multivariate/Univariate Gaussians to model our data.

The **Multivariate Gaussian Distribution** is given by the equation:

![multi_variate](https://miro.medium.com/max/700/1*qUy5tdKD3JF8SBpGfN9TpQ.png)

* **Mu** - Mean
* **Sigma** - Covariance Matrix
* **x** - Data Point/Matrix
* **exp** - Exponential

### Expectation:

![expectation](https://miro.medium.com/max/453/1*4r_oPcLb1i-0Bv8IILKiTg.png)

At this step, the above equation finds the entries to a responsibility matrix, which essentially comprises the probability that a particular value belongs to a particular cluster.

This equation is arrived at using Bayes theorem over the mixture of gaussians.

### Maximization
At this step, the posteriors estimated in the representation matrix are then utilized, and update the mean, variance/covariance estimates such that the log-likelihood is maximized.

These two steps are run iteratively and the values are updated till convergence.

![mean_update](https://miro.medium.com/max/395/0*L9uVNWuLFQVjqtIu)
**Left:** Mean Update, **Right:** Covariance/Variance Update

The evaluation criterion is given by the log-likelihood criterion:

![log_likelihood](https://miro.medium.com/max/576/1*YOEEcpEOWLyeVvdq6ePJ0g.png)

To run the algorithm, you can just modify the test.py

1. **Number of features** - Ensure your data is in the format (n_samples, d). 
   
   For example, if you have 100 samples of 2-d data (x, y), (100, 2).
2. **Number of Clusters (num_clusters (start with 2))** - Number of candidate clusters (gaussians) you would like to fit to model your data.
3. **Number of Iterations (num_iters)** - Number of iterations to run EM (start with 100, till the Log-likelihood converges).

![figure_1.png](assets/Figure_1.png)

# References and Further Reading:
* [Gaussian Mixture Models Explained (With Derivation)](https://towardsdatascience.com/gaussian-mixture-models-explained-6986aaf5a95)
* [Gaussian Mixture Modelling (GMM)](https://towardsdatascience.com/gaussian-mixture-modelling-gmm-833c88587c7f)
* [Gaussian Mixture Models - Stanford](http://statweb.stanford.edu/~tibs/stat315a/LECTURES/em.pdf)
* [CSAIL Lecture - GMM and EM](http://people.csail.mit.edu/dsontag/courses/ml12/slides/lecture21.pdf)
* [Brilliant.org Gaussian Mixture Model](https://brilliant.org/wiki/gaussian-mixture-model/)
* [Gaussian Mixture Models implemented from scratch](https://towardsdatascience.com/gaussian-mixture-models-implemented-from-scratch-1857e40ea566)
